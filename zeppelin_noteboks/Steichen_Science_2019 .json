{"paragraphs":[{"text":"%spark.pyspark\n\n\nimport pandas\nimport re\nfrom pyspark.sql import functions as F\nimport tempfile\nimport os\nimport gzip\nimport shutil\n\n\"\"\"\nImport Parquet As a DataFrame\n\"\"\"\n\nparquet_s3 = \"s3://steichenetalpublicdata/analyzed_sequences/parquet\"\ndf_spark = spark.read.parquet(parquet_s3)\ndf_spark.count()","user":"anonymous","dateUpdated":"2019-11-05T08:44:04+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1112960406\n"}]},"apps":[],"jobName":"paragraph_1572907038729_1263875915","id":"20191104-223718_180465893","dateCreated":"2019-11-04T22:37:18+0000","dateStarted":"2019-11-05T08:44:04+0000","dateFinished":"2019-11-05T08:44:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2106","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-40-186.us-west-2.compute.internal:4040/jobs/job?id=38","http://ip-172-31-40-186.us-west-2.compute.internal:4040/jobs/job?id=39"],"interpreterSettingId":"spark"}}},{"text":"%spark.pyspark\nclass Query():\n    \n    def __init__(self,q_name,length='',v_gene=\"\",d_gene=\"\",j_gene=\"\",regex=\"\"):\n        self.query_name = q_name\n        self.v_gene = v_gene\n        self.j_gene = j_gene\n        self.d_gene = d_gene\n        \n        if not length:\n            raise Exception(\"Length must be supplied\")\n        self.length = length\n        self.regular_expression = regex\n    \n    \n    \n    def apply(self,df):\n        self.queried_dataframe = \"\"\n        \n        ##Lets get length\n        self.queried_dataframe = df.filter(F.length(df.cdr3_aa) == self.length)\n        \n        ##If the rest of these were specified, add them to the filter\n        if self.v_gene:\n            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.v_gene == self.v_gene)\n     \n        if self.d_gene:\n            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.d_gene == self.d_gene)       \n        \n        if self.j_gene:\n            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.j_gene == self.j_gene)       \n        \n\n        if self.regular_expression:\n             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.cdr3_aa.rlike(self.regular_expression))\n            \n            \n        print(\"Found {} sequences\".format(self.queried_dataframe.count()))\n        return self.queried_dataframe\n        \n    \ndef write_out_hadoop_to_local(df, s3):\n    #Output Name\n    output_s3 = s3\n    \n    #A temporary file to write out Hadoop CSV\n    t = tempfile.NamedTemporaryFile(delete=False)\n    \n\n    #Write out the CSV\n    df.write.csv(t.name,mode='overwrite')\n    with open(output_s3,'w') as f:\n        f.write(\",\".join(df.columns)+'\\n')\n        \n        print(\"Running HDFS concatenate\\n{} > {}\".format(' '.join(['hdfs','dfs','-cat','{}/*csv'.format(t.name)]),output_s3))\n        subprocess.Popen(['hdfs','dfs','-cat','{}/*csv'.format(t.name)],stdout=f)\n        \n\n    \n    print('Wrote CSV to {}'.format(output_s3))\n\n\n\n#query is IGHD3-3/J6 of length 23 with an FGV motif at the 7th index position of the HCDR3\nmy_query = Query('BG18 D3-3',d_gene='IGHD3-3',j_gene='IGHJ6',length=23,regex='^.......FGV.............$')\n\n#To run query, pass the input object from above to apply\nqueried_df = my_query.apply(df_spark)\n\n#To get an output, we can write the matching antibodies as a CSV file to our local file system from the Hadoop File System \nwrite_out_hadoop_to_s3(final_df,'/home/hadoop/output/mycsvoutput.csv')","user":"anonymous","dateUpdated":"2019-11-05T08:44:08+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 39973 sequences\nRunning HDFS concatenate\nhdfs dfs -cat /tmp/tmpCl6OTI/*csv > /home/hadoop/output/mycsvoutput.csv\nWrote CSV to /home/hadoop/output/mycsvoutput.csv\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-40-186.us-west-2.compute.internal:4040/jobs/job?id=40","http://ip-172-31-40-186.us-west-2.compute.internal:4040/jobs/job?id=41"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1572909099006_-1207764676","id":"20191104-231139_1913046588","dateCreated":"2019-11-04T23:11:39+0000","dateStarted":"2019-11-05T08:44:08+0000","dateFinished":"2019-11-05T08:48:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2107"},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2019-11-04T22:58:55+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1572908335322_1942975770","id":"20191104-225855_2035322857","dateCreated":"2019-11-04T22:58:55+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:2116"}],"name":"Steichen et al Science 2019","id":"2ET81JDSQ","noteParams":{},"noteForms":{},"angularObjects":{"python:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}