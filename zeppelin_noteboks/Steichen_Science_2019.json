{"paragraphs":[{"text":"%spark.pyspark\n\nimport pandas\nimport re\nimport tempfile\nimport os\nimport gzip\nimport shutil\nimport subprocess\n\nfrom pyspark.sql import functions as F\n\n\n\"\"\"\nImport Parquet As a DataFrame\n\"\"\"\n\n##Read in parquet file from public S3 bucket\nparquet_s3 = \"s3://steichenetalpublicdata/analyzed_sequences/parquet\"\ndf_spark = spark.read.parquet(parquet_s3)\n\n##Verify count \ndf_spark.count()","user":"anonymous","dateUpdated":"2019-11-05T23:13:51+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1112960406\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-39-163.us-west-2.compute.internal:4040/jobs/job?id=7","http://ip-172-31-39-163.us-west-2.compute.internal:4040/jobs/job?id=8"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1572907038729_1263875915","id":"20191104-223718_180465893","dateCreated":"2019-11-04T22:37:18+0000","dateStarted":"2019-11-05T23:13:52+0000","dateFinished":"2019-11-05T23:14:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1763"},{"text":"%spark.pyspark\nclass Query():\n    \n    '''An example query class to hold query parameters'''\n    \n    def __init__(self,q_name,length='',v_gene=\"\",d_gene=\"\",j_gene=\"\",regex=\"\"):\n        self.query_name = q_name\n        self.v_gene = v_gene\n        self.j_gene = j_gene\n        self.d_gene = d_gene\n        \n        if not length:\n            raise Exception(\"Length must be supplied\")\n        self.length = length\n        self.regular_expression = regex\n    \n    \n    \n    def apply(self,df):\n        \n        '''Apply function will take in spark dataframe and apply query parameters to it if they exist\n        \n           Returns a filtered dataframe\n        '''\n        self.queried_dataframe = \"\"\n        \n        ##Lets get length\n        self.queried_dataframe = df.filter(F.length(df.cdr3_aa) == self.length)\n        \n        ##If the rest of these were specified, add them to the filter\n        if self.v_gene:\n            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.v_gene == self.v_gene)\n     \n        if self.d_gene:\n            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.d_gene == self.d_gene)       \n        \n        if self.j_gene:\n            self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.j_gene == self.j_gene)       \n        \n\n        if self.regular_expression:\n             self.queried_dataframe = self.queried_dataframe.filter(self.queried_dataframe.cdr3_aa.rlike(self.regular_expression))\n            \n            \n        print(\"Found {} sequences\".format(self.queried_dataframe.count()))\n        return self.queried_dataframe\n        \n    \ndef write_out_hadoop_to_local(df, s3):\n    '''Because spark is distributed, writing out results is done by each worker node. As a result, it breaks up the results into partions. \n    \n    This function will search the hadoop file system and concatenate the parted file back to a single CSV file\n    \n    params df - the quried dataframe\n    s3 - the local file path (make sure its chmod 777)\n    '''\n    \n    #Output Name\n    output_s3 = s3\n    \n    #A temporary file to write out Hadoop CSV\n    t = tempfile.NamedTemporaryFile(delete=False)\n    \n\n    #Write out the CSV\n    df.write.csv(t.name,mode='overwrite')\n    with open(output_s3,'w') as f:\n        f.write(\",\".join(df.columns)+'\\n')\n        \n        print(\"Running HDFS concatenate\\n{} > {}\".format(' '.join(['hdfs','dfs','-cat','{}/*csv'.format(t.name)]),output_s3))\n        subprocess.Popen(['hdfs','dfs','-cat','{}/*csv'.format(t.name)],stdout=f)\n        \n\n    \n    print('Wrote CSV to {}'.format(output_s3))\n\n\n\n#query is IGHD3-3/J6 of length 23 with an FGV motif at the 7th index position of the HCDR3\nmy_query = Query('BG18 D3-3',d_gene='IGHD3-3',j_gene='IGHJ6',length=23,regex='^.......FGV.............$')\n\n#To run query, pass the input object from above to apply\nqueried_df = my_query.apply(df_spark)\n\n#To get an output, we can write the matching antibodies as a CSV file to our local file system from the Hadoop File System \nwrite_out_hadoop_to_local(queried_df,'/home/hadoop/output/mycsvoutput.csv')","user":"anonymous","dateUpdated":"2019-11-05T23:14:05+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-39-163.us-west-2.compute.internal:4040/jobs/job?id=9","http://ip-172-31-39-163.us-west-2.compute.internal:4040/jobs/job?id=10"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1572909099006_-1207764676","id":"20191104-231139_1913046588","dateCreated":"2019-11-04T23:11:39+0000","dateStarted":"2019-11-05T23:14:06+0000","dateFinished":"2019-11-05T23:18:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1764","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 39973 sequences\nRunning HDFS concatenate\nhdfs dfs -cat /tmp/tmpFRAWAf/*csv > /home/hadoop/output/mycsvoutput.csv\nWrote CSV to /home/hadoop/output/mycsvoutput.csv\n"}]}},{"text":"%spark.pyspark\n","user":"anonymous","dateUpdated":"2019-11-05T22:03:35+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1572991415912_-216155390","id":"20191105-220335_643085726","dateCreated":"2019-11-05T22:03:35+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1765"}],"name":"Steichen et al Science 2019","id":"2ET81JDSQ","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}